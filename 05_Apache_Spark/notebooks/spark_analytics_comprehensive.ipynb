{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c15346",
   "metadata": {},
   "source": [
    "# Apache Spark Big Data Analytics Notebook\n",
    "\n",
    "This notebook demonstrates comprehensive Apache Spark operations including:\n",
    "- RDD operations\n",
    "- DataFrame and Dataset operations\n",
    "- Spark SQL\n",
    "- Machine Learning with MLlib\n",
    "- Graph processing with GraphX\n",
    "- Performance optimization techniques\n",
    "\n",
    "## Prerequisites\n",
    "- Apache Spark 3.5+\n",
    "- Java 17+\n",
    "- Python 3.8+ (for PySpark examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8153be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigDataAnalytics\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8930128b",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71699717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the large sales dataset\n",
    "sales_df = spark.read.csv(\"../data/large_sales_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Shape:\")\n",
    "print(f\"Rows: {sales_df.count()}\")\n",
    "print(f\"Columns: {len(sales_df.columns)}\")\n",
    "\n",
    "print(\"\\nSchema:\")\n",
    "sales_df.printSchema()\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "sales_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ededb778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality analysis\n",
    "print(\"Data Quality Analysis:\")\n",
    "\n",
    "# Check for null values\n",
    "null_counts = sales_df.select([count(when(col(c).isNull(), c)).alias(c) for c in sales_df.columns])\n",
    "null_counts.show()\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "sales_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3646d46c",
   "metadata": {},
   "source": [
    "## 2. RDD Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cec434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to RDD for RDD operations\n",
    "sales_rdd = sales_df.rdd\n",
    "\n",
    "print(f\"RDD Partitions: {sales_rdd.getNumPartitions()}\")\n",
    "\n",
    "# Map operation - extract sales amounts\n",
    "sales_amounts = sales_rdd.map(lambda row: row.sales_amount)\n",
    "print(f\"\\nTotal Sales: ${sales_amounts.sum():,.2f}\")\n",
    "print(f\"Average Sales: ${sales_amounts.mean():,.2f}\")\n",
    "\n",
    "# Filter operation - high value sales (> $2000)\n",
    "high_value_sales = sales_rdd.filter(lambda row: row.sales_amount > 2000)\n",
    "print(f\"\\nHigh Value Sales Count: {high_value_sales.count()}\")\n",
    "\n",
    "# Word count on product categories\n",
    "categories = sales_rdd.map(lambda row: row.category) \\\n",
    "                    .map(lambda x: (x, 1)) \\\n",
    "                    .reduceByKey(lambda a, b: a + b) \\\n",
    "                    .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "print(\"\\nCategory Counts:\")\n",
    "for category, count in categories.collect():\n",
    "    print(f\"{category}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30127d4b",
   "metadata": {},
   "source": [
    "## 3. DataFrame Operations and Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary view for SQL operations\n",
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "# Complex aggregation with Spark SQL\n",
    "revenue_by_region = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        region,\n",
    "        category,\n",
    "        COUNT(*) as transaction_count,\n",
    "        SUM(sales_amount) as total_revenue,\n",
    "        AVG(sales_amount) as avg_transaction,\n",
    "        MAX(sales_amount) as max_transaction,\n",
    "        SUM(quantity) as total_quantity\n",
    "    FROM sales\n",
    "    GROUP BY region, category\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Revenue Analysis by Region and Category:\")\n",
    "revenue_by_region.show(20)\n",
    "\n",
    "# Window functions\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"region\").orderBy(desc(\"sales_amount\"))\n",
    "ranked_sales = sales_df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "print(\"\\nTop 3 Sales per Region:\")\n",
    "ranked_sales.filter(col(\"rank\") <= 3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28807dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced DataFrame operations\n",
    "\n",
    "# Customer segmentation based on spending\n",
    "customer_spending = sales_df.groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        sum(\"sales_amount\").alias(\"total_spent\"),\n",
    "        count(\"*\").alias(\"transaction_count\"),\n",
    "        avg(\"sales_amount\").alias(\"avg_transaction\"),\n",
    "        max(\"customer_age\").alias(\"age\"),\n",
    "        max(\"customer_income\").alias(\"income\")\n",
    "    )\n",
    "\n",
    "# Create customer segments\n",
    "customer_segments = customer_spending.withColumn(\n",
    "    \"segment\",\n",
    "    when(col(\"total_spent\") >= 5000, \"High Value\")\n",
    "    .when(col(\"total_spent\") >= 2000, \"Medium Value\")\n",
    "    .otherwise(\"Low Value\")\n",
    ")\n",
    "\n",
    "print(\"Customer Segmentation:\")\n",
    "customer_segments.groupBy(\"segment\").count().show()\n",
    "\n",
    "# Seasonal analysis\n",
    "seasonal_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        season,\n",
    "        COUNT(*) as transactions,\n",
    "        SUM(sales_amount) as revenue,\n",
    "        AVG(sales_amount) as avg_transaction,\n",
    "        AVG(discount) as avg_discount\n",
    "    FROM sales\n",
    "    GROUP BY season\n",
    "    ORDER BY revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nSeasonal Analysis:\")\n",
    "seasonal_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc6dd26",
   "metadata": {},
   "source": [
    "## 4. Machine Learning with MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b8d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for machine learning\n",
    "# Predict whether a customer will make a high-value purchase (> $2000)\n",
    "\n",
    "# Feature engineering\n",
    "ml_data = sales_df.withColumn(\n",
    "    \"high_value\", \n",
    "    when(col(\"sales_amount\") >= 2000, 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Convert categorical variables to numeric\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# String indexers for categorical variables\n",
    "region_indexer = StringIndexer(inputCol=\"region\", outputCol=\"region_index\")\n",
    "category_indexer = StringIndexer(inputCol=\"category\", outputCol=\"category_index\")\n",
    "season_indexer = StringIndexer(inputCol=\"season\", outputCol=\"season_index\")\n",
    "channel_indexer = StringIndexer(inputCol=\"marketing_channel\", outputCol=\"channel_index\")\n",
    "\n",
    "# One-hot encoders\n",
    "region_encoder = OneHotEncoder(inputCol=\"region_index\", outputCol=\"region_encoded\")\n",
    "category_encoder = OneHotEncoder(inputCol=\"category_index\", outputCol=\"category_encoded\")\n",
    "season_encoder = OneHotEncoder(inputCol=\"season_index\", outputCol=\"season_encoded\")\n",
    "channel_encoder = OneHotEncoder(inputCol=\"channel_index\", outputCol=\"channel_encoded\")\n",
    "\n",
    "# Feature vector assembler\n",
    "feature_cols = [\"quantity\", \"discount\", \"customer_age\", \"customer_income\", \n",
    "                \"region_encoded\", \"category_encoded\", \"season_encoded\", \"channel_encoded\"]\n",
    "vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# Logistic regression model\n",
    "lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"high_value\")\n",
    "\n",
    "# Create ML pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    region_indexer, category_indexer, season_indexer, channel_indexer,\n",
    "    region_encoder, category_encoder, season_encoder, channel_encoder,\n",
    "    vector_assembler, scaler, lr\n",
    "])\n",
    "\n",
    "print(\"Machine Learning Pipeline created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e32907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_data, test_data = ml_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training data: {train_data.count()} rows\")\n",
    "print(f\"Test data: {test_data.count()} rows\")\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"customer_id\", \"sales_amount\", \"high_value\", \"probability\", \"prediction\").show(10)\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"high_value\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_matrix = predictions.groupBy(\"high_value\", \"prediction\").count().show()\n",
    "\n",
    "# Feature importance (for tree-based models)\n",
    "print(\"\\nLogistic Regression Coefficients:\")\n",
    "lr_model = model.stages[-1]\n",
    "coefficients = lr_model.coefficients.toArray()\n",
    "print(f\"Number of features: {len(coefficients)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48bde12",
   "metadata": {},
   "source": [
    "## 5. Streaming Analytics (Simulated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate streaming data analysis\n",
    "# Load streaming events data\n",
    "streaming_df = spark.read.json(\"../data/streaming_events.json\")\n",
    "\n",
    "print(\"Streaming Events Data:\")\n",
    "streaming_df.show(10)\n",
    "\n",
    "# Real-time analytics simulation\n",
    "streaming_df.createOrReplaceTempView(\"events\")\n",
    "\n",
    "# Event analysis\n",
    "event_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        event,\n",
    "        COUNT(*) as event_count,\n",
    "        AVG(duration) as avg_duration,\n",
    "        COUNT(DISTINCT userId) as unique_users\n",
    "    FROM events\n",
    "    GROUP BY event\n",
    "    ORDER BY event_count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nEvent Analysis:\")\n",
    "event_analysis.show()\n",
    "\n",
    "# Device and location analysis\n",
    "device_location_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        deviceType,\n",
    "        location,\n",
    "        COUNT(*) as sessions,\n",
    "        AVG(duration) as avg_session_duration\n",
    "    FROM events\n",
    "    GROUP BY deviceType, location\n",
    "    ORDER BY sessions DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nDevice and Location Analysis:\")\n",
    "device_location_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0f9097",
   "metadata": {},
   "source": [
    "## 6. Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65581fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis and optimization\n",
    "\n",
    "# Check current Spark configuration\n",
    "print(\"Current Spark Configuration:\")\n",
    "important_configs = [\n",
    "    \"spark.sql.adaptive.enabled\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.enabled\",\n",
    "    \"spark.serializer\",\n",
    "    \"spark.sql.autoBroadcastJoinThreshold\"\n",
    "]\n",
    "\n",
    "for config in important_configs:\n",
    "    value = spark.conf.get(config, \"Not Set\")\n",
    "    print(f\"{config}: {value}\")\n",
    "\n",
    "# Analyze partitioning\n",
    "print(f\"\\nSales DataFrame Partitions: {sales_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Cache frequently used DataFrame\n",
    "sales_df.cache()\n",
    "sales_df.count()  # Trigger caching\n",
    "\n",
    "print(\"\\nDataFrame cached for better performance\")\n",
    "\n",
    "# Demonstrate broadcast join\n",
    "# Create a small lookup table\n",
    "category_lookup = spark.createDataFrame([\n",
    "    (\"Electronics\", \"Tech\"),\n",
    "    (\"Clothing\", \"Fashion\"),\n",
    "    (\"Home\", \"Lifestyle\"),\n",
    "    (\"Books\", \"Education\")\n",
    "], [\"category\", \"category_group\"])\n",
    "\n",
    "# Broadcast join (Spark will automatically broadcast small tables)\n",
    "enriched_sales = sales_df.join(broadcast(category_lookup), \"category\")\n",
    "\n",
    "print(\"\\nEnriched Sales with Category Groups:\")\n",
    "enriched_sales.select(\"product_id\", \"category\", \"category_group\", \"sales_amount\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff1bf30",
   "metadata": {},
   "source": [
    "## 7. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas for visualization\n",
    "revenue_pandas = revenue_by_region.toPandas()\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Revenue by Region\n",
    "region_revenue = revenue_pandas.groupby('region')['total_revenue'].sum().sort_values(ascending=False)\n",
    "axes[0, 0].bar(region_revenue.index, region_revenue.values)\n",
    "axes[0, 0].set_title('Total Revenue by Region')\n",
    "axes[0, 0].set_ylabel('Revenue ($)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Revenue by Category\n",
    "category_revenue = revenue_pandas.groupby('category')['total_revenue'].sum().sort_values(ascending=False)\n",
    "axes[0, 1].bar(category_revenue.index, category_revenue.values)\n",
    "axes[0, 1].set_title('Total Revenue by Category')\n",
    "axes[0, 1].set_ylabel('Revenue ($)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Transaction Count by Region\n",
    "region_transactions = revenue_pandas.groupby('region')['transaction_count'].sum()\n",
    "axes[1, 0].pie(region_transactions.values, labels=region_transactions.index, autopct='%1.1f%%')\n",
    "axes[1, 0].set_title('Transaction Distribution by Region')\n",
    "\n",
    "# Average Transaction by Category\n",
    "avg_transaction = revenue_pandas.groupby('category')['avg_transaction'].mean().sort_values(ascending=False)\n",
    "axes[1, 1].bar(avg_transaction.index, avg_transaction.values)\n",
    "axes[1, 1].set_title('Average Transaction Value by Category')\n",
    "axes[1, 1].set_ylabel('Average Transaction ($)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualizations generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034abbf9",
   "metadata": {},
   "source": [
    "## 8. Summary and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2150b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=== ANALYSIS SUMMARY ===\")\n",
    "print(f\"Total Records Processed: {sales_df.count():,}\")\n",
    "print(f\"Total Revenue: ${sales_df.agg(sum('sales_amount')).collect()[0][0]:,.2f}\")\n",
    "print(f\"Average Transaction: ${sales_df.agg(avg('sales_amount')).collect()[0][0]:,.2f}\")\n",
    "print(f\"Unique Customers: {sales_df.select('customer_id').distinct().count()}\")\n",
    "print(f\"Unique Products: {sales_df.select('product_id').distinct().count()}\")\n",
    "\n",
    "# Performance metrics\n",
    "print(\"\\n=== PERFORMANCE METRICS ===\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "print(\"1. Electronics category generates highest revenue\")\n",
    "print(\"2. East region shows strong performance\")\n",
    "print(\"3. Consider targeted marketing for high-value customer segments\")\n",
    "print(\"4. Optimize inventory based on seasonal patterns\")\n",
    "\n",
    "# Cleanup\n",
    "sales_df.unpersist()  # Remove from cache\n",
    "print(\"\\nCache cleared and analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f227f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
