================================================================================
                        JAVA FUNDAMENTALS FOR BIG DATA
================================================================================

WEEK 1-2: Core Java Concepts for Big Data Processing

================================================================================
1. JAVA COLLECTIONS FRAMEWORK FOR BIG DATA
================================================================================

1.1 UNDERSTANDING COLLECTIONS IN BIG DATA CONTEXT
---------------------------------------------
• ArrayList vs LinkedList performance in large datasets
• HashMap vs TreeMap for key-value operations
• HashSet vs TreeSet for unique data handling
• Thread-safe collections for concurrent processing

PERFORMANCE COMPARISON:
Operation          ArrayList    LinkedList    HashMap    TreeMap
Access (get)       O(1)         O(n)          O(1)       O(log n)
Insert (middle)    O(n)         O(1)          O(1)       O(log n)
Search             O(n)         O(n)          O(1)       O(log n)
Memory Usage       Lower        Higher        Medium     Higher

1.2 BEST PRACTICES FOR BIG DATA:
------------------------------
• Use ArrayList for frequent random access
• Use LinkedList for frequent insertions/deletions
• Use HashMap for fast lookups in large datasets
• Use TreeMap when you need sorted order
• Always specify initial capacity for better performance

Example:
// For large datasets, specify initial capacity
List<String> bigDataList = new ArrayList<>(1000000);
Map<String, Object> dataMap = new HashMap<>(500000);

================================================================================
2. JAVA STREAM API FOR DATA PROCESSING
================================================================================

2.1 STREAM OPERATIONS FOR BIG DATA
---------------------------------
Streams provide functional programming approach for data processing:

• Intermediate Operations: filter, map, flatMap, sorted, distinct
• Terminal Operations: collect, reduce, forEach, count, min, max

2.2 PRACTICAL EXAMPLES:

// Processing large datasets with streams
List<Transaction> transactions = loadLargeDataset();

// Filter and transform data
List<String> highValueCustomers = transactions.stream()
    .filter(t -> t.getAmount() > 10000)
    .map(Transaction::getCustomerId)
    .distinct()
    .collect(Collectors.toList());

// Parallel processing for better performance
double averageAmount = transactions.parallelStream()
    .mapToDouble(Transaction::getAmount)
    .average()
    .orElse(0.0);

// Grouping data (similar to SQL GROUP BY)
Map<String, List<Transaction>> byCustomer = transactions.stream()
    .collect(Collectors.groupingBy(Transaction::getCustomerId));

2.3 STREAM PERFORMANCE TIPS:
---------------------------
• Use parallel streams for CPU-intensive operations on large datasets
• Avoid parallel streams for I/O operations
• Use primitive streams (IntStream, DoubleStream) for better performance
• Combine operations to reduce intermediate collections

================================================================================
3. MULTITHREADING AND CONCURRENCY
================================================================================

3.1 THREAD MANAGEMENT FOR BIG DATA
---------------------------------
Big Data applications often require concurrent processing:

• ExecutorService for thread pool management
• CompletableFuture for asynchronous processing
• Thread-safe collections
• Synchronization mechanisms

3.2 EXECUTOR SERVICE EXAMPLE:

import java.util.concurrent.*;

public class BigDataProcessor {
    private final ExecutorService executor = 
        Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors());
    
    public void processLargeDataset(List<DataChunk> chunks) {
        List<Future<ProcessResult>> futures = new ArrayList<>();
        
        for (DataChunk chunk : chunks) {
            Future<ProcessResult> future = executor.submit(() -> {
                // Process each chunk in parallel
                return processChunk(chunk);
            });
            futures.add(future);
        }
        
        // Collect results
        for (Future<ProcessResult> future : futures) {
            try {
                ProcessResult result = future.get();
                // Handle result
            } catch (Exception e) {
                // Handle exception
            }
        }
    }
}

3.3 COMPLETABLEFUTURE FOR ASYNC PROCESSING:

CompletableFuture<String> dataProcessing = CompletableFuture
    .supplyAsync(() -> loadData())
    .thenApply(data -> transformData(data))
    .thenApply(transformed -> analyzeData(transformed))
    .thenCompose(analysis -> saveResults(analysis));

================================================================================
4. MEMORY MANAGEMENT FOR BIG DATA
================================================================================

4.1 JVM MEMORY TUNING
--------------------
Big Data applications require careful memory management:

JVM Parameters for Big Data:
-Xms8g                    # Initial heap size
-Xmx16g                   # Maximum heap size
-XX:NewRatio=3            # Young generation ratio
-XX:+UseG1GC              # Use G1 garbage collector
-XX:MaxGCPauseMillis=200  # Maximum GC pause time
-XX:+UseCompressedOops    # Compress object pointers

4.2 MEMORY-EFFICIENT PROGRAMMING:
-------------------------------
• Use primitive collections (TIntArrayList, TLongHashSet)
• Avoid creating unnecessary objects in loops
• Use StringBuilder for string concatenation
• Implement object pooling for frequently used objects
• Use weak references for caches

Example of memory-efficient code:
// Instead of this:
List<Integer> numbers = new ArrayList<>();
for (int i = 0; i < 1000000; i++) {
    numbers.add(i);  // Boxing creates Integer objects
}

// Use this:
TIntArrayList numbers = new TIntArrayList(1000000);
for (int i = 0; i < 1000000; i++) {
    numbers.add(i);  // No boxing, direct primitive storage
}

================================================================================
5. EXCEPTION HANDLING IN BIG DATA APPLICATIONS
================================================================================

5.1 ROBUST ERROR HANDLING:
-------------------------
• Use try-with-resources for automatic resource management
• Implement circuit breaker patterns for external services
• Log errors with proper context for debugging
• Use custom exceptions for domain-specific errors

5.2 EXAMPLE:

public class DataProcessor {
    private static final Logger logger = LoggerFactory.getLogger(DataProcessor.class);
    
    public void processFile(String fileName) {
        try (BufferedReader reader = Files.newBufferedReader(Paths.get(fileName))) {
            String line;
            int lineNumber = 0;
            
            while ((line = reader.readLine()) != null) {
                lineNumber++;
                try {
                    processLine(line);
                } catch (DataProcessingException e) {
                    logger.error("Error processing line {} in file {}: {}", 
                               lineNumber, fileName, e.getMessage());
                    // Continue processing other lines
                }
            }
        } catch (IOException e) {
            logger.error("Error reading file {}: {}", fileName, e.getMessage());
            throw new DataProcessingException("Failed to process file: " + fileName, e);
        }
    }
}

================================================================================
6. PRACTICAL EXERCISES
================================================================================

EXERCISE 1: Large Dataset Processing
-----------------------------------
Create a program that:
1. Reads a CSV file with 1 million records
2. Filters records based on criteria
3. Groups data by category
4. Calculates statistics (avg, min, max, count)
5. Writes results to output file

EXERCISE 2: Concurrent Data Processing
------------------------------------
Implement a multi-threaded application that:
1. Divides large dataset into chunks
2. Processes each chunk in parallel
3. Aggregates results from all threads
4. Handles exceptions gracefully

EXERCISE 3: Memory Optimization
-----------------------------
Compare memory usage between:
1. Regular collections vs primitive collections
2. String concatenation vs StringBuilder
3. Different garbage collection strategies

================================================================================
7. ASSESSMENT CHECKLIST
================================================================================
□ Understand when to use different collection types
□ Can write efficient stream operations
□ Know how to implement parallel processing
□ Understand JVM memory management
□ Can handle exceptions in data processing scenarios
□ Familiar with performance optimization techniques

================================================================================
NEXT MODULE: 02_Spring_Boot_Basics
================================================================================
Once you've mastered these Java fundamentals, you'll be ready to learn
Spring Boot for building robust Big Data applications.

Practice these concepts daily and ensure you understand each topic before
moving to the next module!
